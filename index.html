<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673183-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673183-1');
</script>

  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/icon.png"> -->
  <title>Shivanand Venkanna Sheshappanavar</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <!-- Bio and Image -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Shivanand Venkanna Sheshappanavar</name>
              </p>
		  
	      <p>&nbsp;</p>
<!-- 	      <p align="justify" style="color:red;">
	       <b><span color="red">
		 I actively seek self-motivated students interested in 3D Computer Vision and Deep Learning. If you are interested, please email me with your Resume/CV, cover letter/statement of purpose, and transcripts (unofficial)!
	       </span></b>
	      </p> -->
		    
              <p align="justify">
		   Dr. Shivanand Sheshappanavar is a tenure-track Assistant Professor in the Department of Electrical Engineering and Computer Science (EECS) at the University of Wyoming. Dr. Sheshappanavar earned a Ph.D. in Computer and Information Sciences from the University of Delaware, where the doctoral research was conducted in the VIMS Laboratory under the supervision of <a href="https://www.eecis.udel.edu/~chandra/"> Dr. Chandra Kambhamettu</a>. In 2018, Dr. Sheshappanavar completed a Master’s degree in Computer Science at Syracuse University, New York. Prior to that, from 2012 to 2016, Dr. Sheshappanavar worked as an IT Consultant at Oracle India Private Limited. Additionally, Dr. Sheshappanavar holds a Master’s degree in Computer Science and Engineering from RVCE (2012) and a Bachelor’s degree in Computer Science and Engineering from MSRIT (2009) from Bengaluru, Karnataka, India.
	      <p align="justify">
	       Dr. Sheshappanavar’s primary research interests lie in 3D computer vision and large (vision) language models, with applications spanning grocery recognition for the visually impaired, controlled-environment agriculture, and digital humanities.
              </p>
	      
	      <p align="justify"> <strong>
	        Recent News: </strong>
	        <ul>
		  <li> <i>August 2025</i>Invited as Reviewer for <a href="https://3dvconf.github.io/2026/"> 3DV 2026</a>a>
		  <li> <i>August 2025</i>Welcoming three Master's students, Aashish, Bipin, and Prashanna!
		  <li> <i>July 2025</i> Invited as Reviewer for <a href="https://wacv.thecvf.com/">WACV 2026</a>
		  <li> <i>July 2025</i> Invited to serve as Program Committee <a href="https://aaai.org/conference/aaai/aaai-26/">AAAI 2026</a>
	          <li> <i>May 2025</i> Mentoring five undergraduate students under the NSF Summer REU Program.
		  <li> <i>April 2025:</i><b>Extended Abstract Accepted at Women in Computer Vision Workshop at CVPR 2025</b>. Title: "Towards AI-Driven Adaptive Lab Evolution: Fine-Tuning Protein Language Models for Mutational Effects".
		  <li> <i>March 2025:</i>Reviewer Invitation,  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025).
		  <li> <i>January 2025:</i> Invited Talk on "Grocery Recognition for the Visually Impaired" at <a href="https://www.amrita.edu/school/computing/bengaluru/"> Amrita Vishwa Vidyapeetham Bengaluru Campus</a>
		  <li> <i>January 2025:</i> UW Research Excellence Fund Seed Grants (PI, $27k)!!!
		  <li> <i>January 2025:</i> Paper Accepted <b>(Oral)</b> at <a href="https://wacv2025-image-quality-workshop2.github.io/index.html"> 4th Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI, WACV 2025, Tucson, AZ, USA </a>
<!-- 		  <li> <i>November 2024:</i> Invited to serve as Reviewer (<a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>).
		  <li> <i>October 2024:</i> Attended Conference on Language Modeling (<a href="https://colmweb.org/index.html">COLM 2024</a>), Philadelphia.
		  <li> <i>September 2024:</i> Paper Accepted (Oral) at <a href="https://www.icmla-conference.org/icmla24/index.php">23rd IEEE ICMLA, 2024</a> <b>EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation</b>!
		  <li> <i>August 2024:</i> Invited to serve as Reviewer (<a href="https://iclr.cc/">ICLR 2025</a>).
		  <li> <i>August 2024:</i> Invited to serve as Program Committee (<a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a>).
	          <li> <I>August 2024:</i> Welcome to my first master's student <a href="https://joan947.github.io/">Joana Owusu</a>.
		  <li> <i>July 2024:</i> Invited to review papers for the Asian Conference on Computer Vision (<a href="https://accv2024.org/">ACCV 2024</a>).
	          <li> <I>May 2024:</i> Welcome to my first PhD student <a href="http://michaelelgin.com/">Michael Elgin</a>.
		  <li> <I> May 2024: </i> Three years of Student Fellowship Funding secured for research on Controlled Environment Agriculture.
		  <li> <i>April 2024:</i> Invited to review papers for the European Conference on Computer Vision (ECCV) 2024.
		  <li> <i>December 2023:</i> Awarded School of Computing Spring 2024 Faculty Fellow (Single-PI, $30k)!!!.
		  <li> <i>October 2023:</i> One paper accepted in  <a href="https://3dvconf.github.io/2024/">3DV</a> Conference!!!.
		  <li> <i>August 2023:</i> Joined the Department of Electrical Engineering and Computer Science at the University of Wyoming as a Tenure-Track Assistant Professor -->
	        </ul>
              </p>
	    </td>
<!-- 	    <td width="15%">
              <img src='images/self.png' width=100%>
            </td> -->
            <td width="22%">
              <img src="images/selfbg.png" width=100%>
            </td>
          </tr>
	  <tr>
            <td colspan="2">
              <p align=center margin=0px>
                <a href="mailto:ssheshap@uwyo.edu">Email</a> &nbsp/&nbsp
                <a href="data/ShivanandSheshappanavar_CV.pdf">CV</a> &nbsp/&nbsp
		<a href="data/Statement_of_Teaching_Philosophy.pdf">Teaching Philosophy</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=I1ajnioAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/sheshap">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ssheshappanavar/"> LinkedIn </a>  &nbsp/&nbsp
		<a href="https://www.facebook.com/groups/PhDinUS"> PhDinUS (Facebook) </a> &nbsp/&nbsp
		<a href="https://twitter.com/geometricintel" class="twitter-follow-button" data-show-count="false">Follow @GeometricIntel</a> &nbsp/&nbsp
		<a href="https://www.brains-explained.com/advice-for-applying-to-phd-programs/"> PhD Aspirants</a>
             </p>
	     <p>&nbsp;</p>
	   </td>
	  </tr>
        </table>
	<div class = "row">
        <table width="100%" align="center" margin-top=100px>
            <tr>
	      <td align="center" width="16%" style = "vertical-align: middle">
                <img src = "/images/eecs_uwyo_logo.jpg" width="60%">
              </td>
              <td align="center" width="16%" style = "vertical-align: middle">
                <img src = "/images/UD.png" width="60%">
              </td>
		    
	      <td align="center" width="16%" style = "vertical-align: middle">
                <img src = "/images/syracuse.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle">
                <img src = "/images/oracle.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle">
                <img src = "/images/infineon.png" width="60%">
              </td>
            </tr>

            <tr>
		<td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Assistant Professor<br>University of Wyoming<br>2023 - present</td>
		    
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD, CS<br>University of Delaware<br>2018 - 2023</td>
		    
		<td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MS, CS<br>Syracuse University<br>2016 - 2018</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">IT Consultant<br>Oracle<br>2012 - 2016</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>Infineon<br>2011 - 2012</td>

                <!-- <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MTech<br>R.V. College of Engineering<br>2010 - 2012</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">BE<br>M.S.Ramaiah Institute of Technology<br>2005 - 2009</td> -->
          </tr>

          </table>
      </div>


   <!-- Post-doc -->
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	    <tr>
	      <td width="100%" valign="middle">
	        <heading>Post-doc</heading>
	        <p align="justify">
		<a href="https://scholar.google.com/citations?user=hyTlrNQAAAAJ&hl=zh-CN">Xiao Yang</a> (Oct 2025 - incoming) <br>

	        </p>
	      </td>
	    </tr>
        </table>
   <!-- Students -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	    <tr>
	      <td width="100%" valign="middle">
	        <heading>Students</heading>
	        <p align="justify">
		<a href="https://joan947.github.io/">Joana Konadu Owusu</a> (MS in CS, Aug 2024 - present) <br>
		<a href="http://michaelelgin.com/">Michael Elgin</a> (PhD in CS, May 2024 - present) <br>
		<a href="https://aashishpokhrel27.github.io/portfolio/">Aashish Pokhrel</a> (MS in CS, Aug 2025 - present) <br>
		<a href="https://bipin38.github.io/Portfolio/">Bipin Ghimire</a> (MS in CS, Aug 2025 - present) <br>
		<a href="https://pmpaudel.com.np/">Prashanna Paudel</a> (MS in AI, Aug 2025 - present) <br>
	        </p>
	      </td>
	    </tr>
        </table>


   <!-- Resources -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	    <tr>
	      <td width="100%" valign="middle">
	        <heading>Resources</heading>
	        <p align="justify">
		In cluster <br>
		Five nodes of 8xH100 GPUs (total 40 H100 GPUs). <br>
		Six nodes of 8xL40 GPUs (total 48 L40 GPUs). <br>
		Eight nodes of 8xA30 GPUs (total 64 A30 GPUs). <br>
		Lab owned <br>
		One node of 4xA6000 GPUs <br>
		One Workstation of 2xA6000 GPUs <br>
		One Workstation of 4xADA A6000 GPUs. <br>
	        </p>
	      </td>
	    </tr>
        </table>
	
        <!-- Research Interest -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Research</heading>
        <p align="justify">
          My research interests include developing deep learning algorithms for 3D computer vision problems and creating end-to-end solution pipelines. My long-term goal is to develop a wearable assistant for the visually impaired, helping them navigate the real world.
        </p>
      </td>
    </tr>
        </table>

        <!-- Publications-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<!-- GroceryWild500 -->
<!-- 	  <tr>
            <td width="35%">
              <img src='images/GroceryWild500.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="">
                  <papertitle>Grocery in the Wild: A Benchmark Dataset for NeRF and 3D Gaussian Splatting</papertitle>
                </a>
                <br>
                <strong>Shreyas Murugodmath</strong>, Michael Elgin, Shivanand Venkanna Sheshappanavar
		<br>
		<em>4th Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI, WACV 2025, Tucson, AZ, USA</em>
		<br>
		<a href="">[paper coming soon]</a> <a href="">[project page coming soon]</a> <br>
		<p align="justify"> With the advent of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), novel view synthesis has become a central tool for 3D scene analysis. Despite progress, there remains a lack of publicly available datasets in the grocery domain, which features diverse geometry, lighting, materials, and text-rich packaging. We introduce GroceryWild500, a large-scale benchmark of 547 real-world grocery store scenes spanning fruits, vegetables, and packaged goods, organized into 74 semantic categories. Each scene is captured via mobile devices and processed into both COLMAP-style and Nerfstudio-style formats, ensuring compatibility with a wide range of reconstruction pipelines. We benchmark six state-of-the-art NeRF and 3DGS models, complemented by ablation studies on variable image resolution and views, as well as task-based experiments such as segmentation and extended Gaussian Splatting variants. Our results highlight the scale and difficulty of GroceryWild500 compared to canonical datasets. GroceryWild500 establishes a foundation for future research in reconstruction, segmentation, and multimodal scene understanding.</p>
            </td>
        </tr> -->

		<!-- UPD-3D -->
	  <tr>
            <td width="35%">
              <img src='images/UPD-3D_images.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="">
                  <papertitle>Can 3D-LLMs Say “There is no correct answer”? Benchmarking for Unsolvable Problem Detection</papertitle>
                </a>
                <br>
                <strong>Michael Elgin</strong>,Shivanand Venkanna Sheshappanavar
		<br>
<!-- 		<em></em> -->
		<br>
		<a href="">[paper coming soon]</a> <a href="">[code coming soon]</a> <br>
		<p align="justify">Unsolvable Problem Detection (UPD) refers to a Large Language Model’s (LLMs) ability to recognize and refrain from responding to unsolvable or poorly posed questions, a capability that is essential for safe deployment in understanding the real world (3D scenes). Recently, UPD has gained traction in the evaluation of LLMs.  However, UPD has been underexplored in the 3D domain.  Although 2D-LLMs have been evaluated for UPD, similar evaluations for 3D-LLMs remain unexplored due to the novelty of the task and the lack of suitable datasets. To address this gap, we introduce the first UPD-3D benchmark constructed from the popular 3D-GRAND dataset. This benchmark comprises 11,964 scene-level samples for each of the 12 distinct UPD categories, partitioned into roughly 8,971 training and 2,993 testing instances. We establish 3D-UPD baselines by benchmarking two recent 3D-LLMs, namely MiniGPT-3D (ACM MM'24) and GreenPLM (AAAI'25). Evaluation of these 3D-LLMs directly using our test set shows critical failure modes and emphasizes the necessity of incorporating UPD capabilities into future 3D-LLMs. Our training-based evaluations of these 3D-LLMs demonstrated a significant improvement in the performance of these models in more than 50\% of the distinct UPD categories.  </p>
            </td>
        </tr>
			
	<!-- LoRApLM -->
	  <tr>
            <td width="35%">
              <img src='images/LoRApML.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="">
                  <papertitle>Learning Adaptive Lab Evolved Mutational Landscapes: Leveraging LoRA on a Protein Language Model</papertitle>
                </a>
                <br>
                <strong>Silba Dowell</strong>,Shivanand Venkanna Sheshappanavar
		<br>
		<em>Extended Abstract at Women in Computer Vision Workshop, CVPR 2025, Nashville, TN, USA</em>
		<br>
		<a href="https://github.com/sheshap/sheshap.github.io/blob/master/pdf/cvpr25_poster_template_LoRApLM.pdf">[poster]</a> <br>
		<p align="justify">  In adaptive laboratory evolution (ALE), microorganisms are subjected to a range of environmental pressures, such as temperature and radiation, in multiple experimental trials to induce naturally occurring, yet desirable, phenotypes. This approach has paved the way for innovations in protein engineering, yielding improvements in biological properties. However, ALE is constrained by its high demand for intensive resources, including the expertise of highly skilled biologists, costly equipment, and a large volume of biological samples, as well as the continuous oversight required to maintain such experimental rigor. One solution is to leverage protein language models (pLMs) to predict impactful mutations and drive evolutionary trajectories without direct human supervision. In this work, ESM-2 is fine-tuned using Low-Rank Adaptation (LoRA) on Escherichia coli data from the ALEdb database for variant effect prediction.</p>
            </td>
        </tr>
	<!-- MahalanobiskNN -->
	  <tr>
            <td width="35%">
              <img src='images/MahalanobiskNN.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="">
                  <papertitle>Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations</papertitle>
                </a>
                <br>
                <strong>Tejas Anvekar</strong>,Shivanand Venkanna Sheshappanavar
		<br>
		<em>4th Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI, WACV 2025, Tucson, AZ, USA</em>
		<br>
		<a href="https://arxiv.org/pdf/2409.06267">[paper]</a> <a href="https://github.com/TejasAnvekar/Mahalanobis-k-NN">[code]</a> <br>
		<p align="justify"> In this paper, we discuss Mahalanobis k-NN: a statistical lens designed to address the challenges of feature matching in learning-based point cloud registration when confronted with an arbitrary density of point clouds, either in the source or target point cloud. We tackle this by adopting Mahalanobis k-NN's inherent property to capture the distribution of the local neighborhood and surficial geometry. Our method can be seamlessly integrated into any local-graph-based point cloud analysis method. This paper focuses on two distinct methodologies: Deep Closest Point (DCP) and Deep Universal Manifold Embedding (DeepUME). Our extensive benchmarking on the ModelNet40 and Faust datasets highlights the efficacy of the proposed method in point cloud registration tasks. Moreover, we establish for the first time that the features acquired through point cloud registration inherently can possess discriminative capabilities. This is evident by a substantial improvement of about 20% in the average accuracy observed in the point cloud few-shot classification task benchmarked on ModelNet40 and ScanObjectNN.</p>
            </td>
        </tr>

	<!-- EDADepth -->
          <tr>
            <td width="35%">
              <img src='images/EDADepth.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="">
                  <papertitle>EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation </papertitle>
                </a>
                <br>
                <strong>Nischal Khanal</strong>,Shivanand Venkanna Sheshappanavar
		<br>
		<em>2024 23rd International Conference on Machine Learning and Applications (IEEE, 2024), December 2024, Miami, Florida, USA</em>
		<br>
		<a href="https://arxiv.org/pdf/2409.06183">[paper]</a> <a href="https://github.com/edadepthmde/EDADepth_ICMLA">[code]</a> <br>
		<p align="justify"> Due to their text-to-image synthesis feature, diffusion models have recently seen a rise in visual perception tasks, such as depth estimation. The lack of good-quality datasets makes extracting fine-grain semantic context challenging for the diffusion models. The semantic context with fewer details further worsens the process of creating effective text embeddings that will be used as input for diffusion models. This paper proposes a novel EDADepth, an Enhanced Data Augmentation method for Monocular Depth Estimation without using extra training data. We use Swin2SR, a super-resolution model, to enhance the quality of input images. We employ the BEiT pre-trained semantic segmentation model to extract better text embeddings. Furthermore, we introduce the BLIP-2 tokenizer to generate tokens from these text embeddings. The novelty of our approach is the introduction of Swin2SR, BEiT model, and BLIP-2 tokenizer in the diffusion-based pipeline for monocular depth estimation. Our model achieves state-of-the-art results (SOTA) on the d3 metric on both NYUv2 and KITTI datasets. It also achieves results comparable to those of the SOTA models in the RMSE and REL metrics. Finally, we also show improvements in the visualization of the estimated depth compared to the SOTA diffusion-based monocular depth estimation models. </p>
            </td>
        </tr>
		
	<!-- 3DGrocery100 -->
          <tr>
            <td width="35%">
              <img src='images/main.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="http://vims.cis.udel.edu/3DGrocery100/">
                  <papertitle>3DGrocery100: A Benchmark Grocery Dataset of Realworld Point Clouds From Single View</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Tejas Anvekar, Shivanand_Kundargi, Yufan Wang, Chandra Kambhamettu.
		<br>
		<em> 2024 International Conference on 3D Vision (3DV) (IEEE, 2024), March 2024, Davos, Switzerland. </em>
		<br>
		<a href="https://arxiv.org/pdf/2402.07819.pdf">[paper]</a> <a href="http://vims.cis.udel.edu/3DGrocery100/">[project page]</a> <br>
		<p align="justify">We introduce a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes, 10,755 RGB-D images, and 87,898 3D point cloud objects. We benchmark our dataset on six recent state-of-the-art 3D object classification models. 3DGrocery100 is the largest real-world 3D point cloud grocery dataset. </p>
            </td>
          </tr>	
		
	   <!-- Local Neighborhood Features for 3D Classification -->
          <tr>
            <td width="35%">
              <img src='images/PointNeXtLocals.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/pdf/2212.05140.pdf">
                  <papertitle>Local Neighborhood Features for 3D Classification</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Chandra Kambhamettu
		<br>
		<em>22nd Scandinavian Conference In Image Analysis (SCIA), April 2023, Levi Ski Resort (Lapland), Finland. </em>
		<br>
		<a href="https://arxiv.org/pdf/2212.05140.pdf">[paper]</a> <a href="https://github.com/VimsLab/Local3DFeatures">[code]</a> <br>
		<p align="justify">With advances in deep learning model training strategies, the training of Point cloud classification methods is significantly improving. For example, PointNeXt, which adopts prominent training techniques and InvResNet layers into PointNet++, achieves over 7% improvement on the real-world ScanObjectNN dataset. However, most of these models use point coordinates features of neighborhood points mapped to higher dimensional space while ignoring the neighborhood point features computed before feeding to the network layers. In this paper, we revisit the PointNeXt model to study the usage and benefit of such neighborhood point features.</p>
            </td>
          </tr>
		
	  <!-- simpleview++ -->
          <tr>
            <td width="35%">
              <img src='images/simpleviewpp.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://ieeexplore.ieee.org/document/9874679">
                  <papertitle>SimpleView++: Neighborhood Views for Point Cloud Classification</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Chandra Kambhamettu
                <br>
                <em>IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR) 2022</em>
		<br>
		<a href="https://ieeexplore.ieee.org/document/9874679">[paper]</a> <a href="https://github.com/VimsLab/SimpleViewPlusPlus">[code]</a> <a href="https://youtu.be/lLoUiqqJPHg">[video]</a><br>
		<p align="justify">We propose the use of neighbor projections along with object projections to learn finer local structural information. SimpleView++ concatenates features from orthogonal perspective projections at object and neighbor levels with encoded features from the point cloud.</p>
            </td>
          </tr>

	  <!-- patch aug -->
          <tr>
            <td width="35%">
              <img src='images/patchaugment.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Sheshappanavar_PatchAugment_Local_Neighborhood_Augmentation_in_Point_Cloud_Classification_ICCVW_2021_paper.html">
                  <papertitle>PatchAugment: Local Neighborhood Augmentation in Point Cloud Classification</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Vinit Veerendraveer Singh, Chandra Kambhamettu
                <br>
                <em> IEEE/CVF International Conference on Computer Vision (ICCV) Workshops 2021</em>
		<br>
		<a href="https://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Sheshappanavar_PatchAugment_Local_Neighborhood_Augmentation_in_Point_Cloud_Classification_ICCVW_2021_paper.pdf">[paper]</a> <a href="https://github.com/VimsLab/PatchAugment">[code]</a> <a href="https://youtu.be/YqP7UVhwdWQ">[video]</a> <br>
		<p align="justify">Different local neighborhoods on the object surface hold a different amount of geometric complexity. Applying the same data augmentation techniques at the object level is less effective in augmenting local neighborhoods with complex structures. This paper presents PatchAugment, a data augmentation framework to apply different augmentation techniques to the local neighborhoods.</p>
            </td>
          </tr>

	  <!-- dynamic scale -->
          <tr>
            <td width="35%">
              <img src='images/dynamicellipsoid.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://ieeexplore.ieee.org/document/9565556">
                  <papertitle>Dynamic local geometry capture in 3d point cloud classification</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Chandra Kambhamettu
                <br>
                <em>IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR) 2021</em>
		<br>
		<a href="https://ieeexplore.ieee.org/document/9565556">[paper]</a> <a href="https://github.com/VimsLab/DynamicScale">[code]</a> <a href="https://youtu.be/Ev44a02mwCg">[video]</a><br>
                <!-- <a href="https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/"><img src='images/synced.png' height=25px></a> &nbsp; &nbsp;
                <a href="https://papersread.ai/e/non-deep-networks/"><img src='images/podbean.png' height=25px></a> -->
		<p align="justify"> PointNet++ model uses ball querying for local geometry capture in its set abstraction layers. Several models based on single scale grouping of PointNet++ continue to use ball querying with a fixed-radius ball. However, ball lacks orientation and is ineffective in capturing complex or varying geometry proportions from different local neighborhoods on the object surface. We propose a novel technique of dynamically oriented and scaled ellipsoid based on unique local information to capture the local geometry better. We also propose ReducedPointNet++, a single set abstraction based single scale grouping model. </p>
            </td>
          </tr>
          <!-- dilated mesh -->
          <tr>
            <td width="35%">
              <img src='images/dmc.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://ieeexplore.ieee.org/document/9506311">
                  <papertitle>Mesh Classification with Dilated Mesh Convolutions</papertitle>
                </a>
                <br>
                Vinit Veerendraveer Singh, <strong>Shivanand Venkanna Sheshappanavar</strong>, Chandra Kambhamettu
                <br>
								  <em>IEEE International Conference on Image Processing (ICIP)</em> 2021
								<br>
                <!-- <em>NeuRIPS</em> 2020, <span style="color:brown;">Spotlight (Top 4% of submitted papers)</span>
		<br> -->
								<a href="https://ieeexplore.ieee.org/document/9506311">[paper]</a>
                <a href="https://github.com/VimsLab/DMC">[code]</a>
		<!-- <a href="https://drive.google.com/file/d/1lWA2WlJR4itJJrnHMRiZ87-z8akagkH4/view?usp=sharing">[slides]</a> -->

		<a href="https://youtu.be/Jdl71d3oMRE">[video]</a>
                <p align="justify"> In this paper, inspired by dilated convolutions for images, we proffer dilated convolutions for meshes. Our Dilated Mesh Convolution (DMC) unit inflates the kernels' receptive field without increasing the number of learnable parameters. We also propose a Stacked Dilated Mesh Convolution (SDMC) block by stacking DMC units. We accommodated SDMC in MeshNet to classify 3D meshes. </p>
            </td>
          </tr>

          <!-- meshnet++ -->
          <tr>
            <td width="35%">
              <img src='images/meshnet2.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2007.11121">
                  <papertitle>MeshNet++: A Network with a Face</papertitle>
                </a>
                <br>
                Vinit Veerendraveer Singh, <strong>Shivanand Venkanna Sheshappanavar</strong>, Chandra Kambhamettu
                <br>
                <em>29th ACM International Conference on Multimedia (ACM MM Oral)</em> 2021
                <br>
								<a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475468">[paper]</a>
                <a href="https://github.com/VimsLab/MeshNet2">[code]</a>
								<a href="https://youtu.be/xcfnhrYqKac">[video]</a>
                <!-- <a href="data/packit_slides.pptx">[slides]</a> -->
                <p align="justify"> MeshNet is a pioneer in this direction. In this paper, we propose a novel neural network that is substantially deeper than its MeshNet predecessor. This increase in depth is achieved through our specialized convolution and pooling blocks that operate on mesh faces. Our network named MeshNet++ learns local structures at multiple scales and is also robust to shortcomings of mesh decimation. We evaluated it for the shape classification task on various data sets, and results significantly higher than state-of-the-art were observed.</p>
            </td>
          </tr>
	  <!-- ellipsoid -->
          <tr>
            <td width="35%">
              <img src='images/ellipsoid_querying.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2106.05304">
                  <papertitle>A novel local geometry capture in pointnet++ for 3d classification</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Chandra Kambhamettu
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em> 2020
		<br>
								<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w16/Sheshappanavar_A_Novel_Local_Geometry_Capture_in_PointNet_for_3D_Classification_CVPRW_2020_paper.pdf">[paper]</a>
                <a href="https://github.com/VimsLab/EllipsoidQuery">[code]</a>
								<a href="https://youtu.be/OMZKTH85T8c">[video]</a>
		<!-- <a href="https://docs.google.com/presentation/d/1iYC5vDQPsb9nG9QCLwpm5TPL8i1F3sxu/edit?usp=sharing&ouid=116890615761823045471&rtpof=true&sd=true">[slides]</a> -->
		<!-- <a href="https://icml.cc/virtual/2021/poster/9099">[video]</a> -->
                <p align="justify">Few of the recent deep learning models for 3D point sets classification are dependent on how well the model captures the local geometric structures. PointNet++ model was able to extract the local region features from points by ball querying the local neighborhoods. However, ball querying is less effective in capturing local neighborhoods of high curvature surfaces or regions. In this paper, we demonstrate improvement in the 3D classification results by using ellipsoid querying around centroids, capturing more points in the local neighborhood. We extend the ellipsoid querying technique by orienting it in the direction of principal axes of the local neighborhood for better capture of the local geometry. </p>
            </td>
          </tr>

					<tr>
            <td width="35%">
              <img src='images/soil_moisture.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2007.11121">
                  <papertitle>LSTM based Soil Moisture Prediction</papertitle>
                </a>
                <br>
                <strong>Shivanand Venkanna Sheshappanavar</strong>, Chilukuri K. Mohan, David G. Chandler
                <br>
                <em>1st Northeast Regional Conference on Complex Systems (NERCCS)</em> 2018
                <br>
								<a href="https://github.com/sheshap/sheshap.github.io/blob/master/pdf/SoilMoisturePrediction_LSTM_NERCCS_2018.pdf">[paper]</a>
                <a href="https://github.com/sheshap/SoilMoisturePrediction">[code]</a>
								<!-- <a href="https://youtu.be/xcfnhrYqKac">[video]</a> -->
                <!-- <a href="data/packit_slides.pptx">[slides]</a> -->
                <p align="justify"> Soil moisture content is an important variable that has a considerable impact on agricultural processes and practical weather-related concerns such as flooding and drought. We address the problem of predicting soil moisture by applying recurrent neural networks that use Long Short-Term Memory (LSTM) models. The success of our approach is evaluated using a dataset obtained from ground-based sensor infrastructure networks. Feature reduction using a mutual information approach is shown to be more effective than feature extraction using principal component analysis.</p>
            </td>
          </tr>
        </table>

        <!-- Teaching -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading> <a href="https://sheshap.github.io/teaching.html">Teaching</a></heading>
								<p>
									I am preparing the following courses at the University of Wyoming:
										<ul>
											<li> COSC 4010/5010: Introduction to Large Language Models [Fall 2025]</li>
										</ul>
									I have been the Instructor for the following courses at the University of Wyoming:
										<ul>
											<li> COSC 4010/5010: Introduction to Deep Learning [Spring 2025]</li>
											<li> EE 5885/COSC 5010: Advances in Deep Learning [Spring 2025]</li>
											<li> EE 5885/COSC 5010: Advances in 3D Computer Vision [Spring 2024]</li>
											<li>EE/COSC 2150: Computer Organization [Fall 2024, Fall 2023]</li>
										</ul>
									I have been the Instructor for the course below at the University of Delaware:
										<ul>
											<li>CISC210: Introduction to Systems Programming [Summer 2020]</li>
										</ul>
                  I have been the Lead Teaching Assistant for the following course:
                    <ul>
                      <li> CISC210: Introduction to Systems Programming at the University of Delaware[Fall 2022, Spring 2022, Spring 2021, Fall 2020, Spring 2020, Fall 2019, Spring 2019] </li>
                    </ul>
									I have been the Teaching Assistant for the following courses:
	                  <ul>
	                    <li> CISC220: Data Structures at the University of Delaware [Fall 2021] </li>
											<li> CISC101: Principles of Computing at the University of Delaware [Winter 2021] </li>
	                    <li> CISC662: Advanced Computer Architecture at the University of Delaware [Fall 2018]</li>
	                  </ul>
                </p>
		<!-- Some <a href="https:/related.html">related papers</a> to mine. -->
              </td>
            </tr>
        </table>

        <!-- Reference -->
        <p align="right"><a href="https://jonbarron.info/">[Web Cite]</a></p>
      </td>
    </tr>
  </table>
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=dwLb98e9AbmFEevS3BoXkPs2XsruHEpsHIDn9lpN6Zs&cl=ffffff&w=a"></script> -->
	<a href="https://clustrmaps.com/site/1byya"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=dwLb98e9AbmFEevS3BoXkPs2XsruHEpsHIDn9lpN6Zs&cl=ffffff" /></a>
</body>

</html>
